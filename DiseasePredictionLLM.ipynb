{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Pending work:\n",
        "* check with different documents (health reports)\n",
        "* play around with different embeddings\n",
        "* choose a better model and measure performance( checkout johnsnowLabs)\n",
        "* find a way to measure performance\n",
        "* input all health reports to the model\n",
        "* design the conversation between user and model\n",
        "* few shot learning\n",
        "* how to use the medical dataset\n",
        "* check with different parameters\n",
        "* UI implementation"
      ],
      "metadata": {
        "id": "eUTAfLx7m3PC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You have to create a user token in huggingface. From profile, settings. Copy that to the executed block output and press enter"
      ],
      "metadata": {
        "id": "1oTd9CTwlY-z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0jw1zHDlQzM"
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "# Enter your Hugging Face token here\n",
        "hf_token = getpass.getpass(\"Enter your Hugging Face token: \")\n",
        "\n",
        "# Set the HF_TOKEN environment variable\n",
        "os.environ[\"HF_TOKEN\"] = hf_token\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index-llms-huggingface\n",
        "!pip install llama-index-embeddings-huggingface\n",
        "!pip install llama-index ipywidgets\n",
        "!pip install bitsandbytes\n",
        "!pip install accelerate\n",
        "!pip install llama-index\n",
        "!pip install --upgrade streamlit\n",
        "!pip install pyngrok"
      ],
      "metadata": {
        "id": "J8UTEjJAles0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/karan-nanonets/llamaindex-guide/raw/main/bcg-2022-annual-sustainability-report-apr-2023.pdf\n",
        "!wget https://github.com/nandhishtr/DiseasePredictionLLM/blob/main/dataset_folder/health_report_%7B0%7D/health_report_%7B0%7D.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owFJJ-yglkEp",
        "outputId": "0c0c7a70-d1d8-49ba-862a-12909a584d6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-20 17:38:28--  https://github.com/karan-nanonets/llamaindex-guide/raw/main/bcg-2022-annual-sustainability-report-apr-2023.pdf\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/karan-nanonets/llamaindex-guide/main/bcg-2022-annual-sustainability-report-apr-2023.pdf [following]\n",
            "--2024-04-20 17:38:28--  https://raw.githubusercontent.com/karan-nanonets/llamaindex-guide/main/bcg-2022-annual-sustainability-report-apr-2023.pdf\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 19069924 (18M) [application/octet-stream]\n",
            "Saving to: ‘bcg-2022-annual-sustainability-report-apr-2023.pdf’\n",
            "\n",
            "bcg-2022-annual-sus 100%[===================>]  18.19M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-04-20 17:38:29 (135 MB/s) - ‘bcg-2022-annual-sustainability-report-apr-2023.pdf’ saved [19069924/19069924]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Base code"
      ],
      "metadata": {
        "id": "pBu28q8alYFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import subprocess\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from llama_index.core import PromptTemplate\n",
        "from llama_index.core import VectorStoreIndex, get_response_synthesizer\n",
        "from llama_index.core.retrievers import VectorIndexRetriever\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.core.node_parser import SimpleNodeParser\n",
        "from llama_index.core import SimpleDirectoryReader\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import Settings\n",
        "\n",
        "# Model names (make sure you have access on HF)\n",
        "LLAMA2_7B = \"mistralai/Mistral-7B-v0.1\"\n",
        "selected_model = LLAMA2_7B\n",
        "\n",
        "query_wrapper_prompt = PromptTemplate(\"{query_str}\")\n",
        "\n",
        "llm = HuggingFaceLLM(\n",
        "    context_window=2048,\n",
        "    max_new_tokens=1024,\n",
        "    generate_kwargs={\"temperature\": 0.7, \"do_sample\": True},\n",
        "    query_wrapper_prompt=query_wrapper_prompt,\n",
        "    tokenizer_name=selected_model,\n",
        "    model_name=selected_model,\n",
        "    device_map=\"cuda:0\",\n",
        "    # change these settings below depending on your GPU\n",
        "    model_kwargs={\"torch_dtype\": torch.float16, \"load_in_4bit\": True},\n",
        ")\n",
        "\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "Settings.llm = llm\n",
        "Settings.embed_model = embed_model\n",
        "\n",
        "print('Disease prediction bot')\n",
        "\n",
        "reader = SimpleDirectoryReader(\n",
        "    input_files=[\"health_report_{0}.txt\"]\n",
        ")\n",
        "\n",
        "text_documents = reader.load_data()\n",
        "parser = SimpleNodeParser.from_defaults(chunk_size=1024, chunk_overlap=20)\n",
        "\n",
        "text_nodes = parser.get_nodes_from_documents(text_documents)\n",
        "index = VectorStoreIndex(text_nodes)\n",
        "\n",
        "# # Clone the GitHub repository\n",
        "# repo_url = \"https://github.com/nandhishtr/DiseasePredictionLLM.git\"\n",
        "# repo_directory = \"medical_reports\"\n",
        "# subprocess.run([\"git\", \"clone\", repo_url, repo_directory], check=True)\n",
        "# # Read text files from the cloned repository\n",
        "# text_documents = []\n",
        "# for i in range(1):  # Assuming there are 100 health reports\n",
        "#     report_folder = os.path.join(repo_directory, \"dataset_folder\", f\"health_report_{i}\")\n",
        "#     report_file = os.path.join(report_folder, f\"health_report_{i}.txt\")\n",
        "#     if os.path.exists(report_file):\n",
        "#         with open(report_file, \"r\") as f:\n",
        "#             text_documents.append(f.read())\n",
        "\n",
        "# parser = SimpleNodeParser.from_defaults(chunk_size=1024, chunk_overlap=20)\n",
        "\n",
        "# # Parse text documents into nodes\n",
        "# text_nodes = parser.get_nodes_from_texts(text_documents)\n",
        "# index = VectorStoreIndex(text_nodes)\n",
        "\n",
        "# configure retriever\n",
        "retriever = VectorIndexRetriever(\n",
        "    index=index,\n",
        "    similarity_top_k=2,\n",
        ")\n",
        "# configure response synthesizer\n",
        "response_synthesizer = get_response_synthesizer(\n",
        "    response_mode=\"simple_summarize\",\n",
        ")\n",
        "# assemble query engine\n",
        "query_engine = RetrieverQueryEngine(\n",
        "    retriever=retriever,\n",
        "    response_synthesizer=response_synthesizer,\n",
        ")\n",
        "\n",
        "# while True:\n",
        "prompt = input('User: ')\n",
        "response = query_engine.query(prompt)\n",
        "print('Bot:', response)\n"
      ],
      "metadata": {
        "id": "A7k1lxT4ltsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Previously tried code, along with streamlit"
      ],
      "metadata": {
        "id": "DhVIhX0plzIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from llama_index.core import PromptTemplate\n",
        "import streamlit as st\n",
        "from pyngrok import ngrok\n",
        "\n",
        "try:\n",
        "    # Model names (make sure you have access on HF)\n",
        "    LLAMA2_7B = \"mistralai/Mistral-7B-v0.1\"\n",
        "    selected_model = LLAMA2_7B\n",
        "\n",
        "    query_wrapper_prompt = PromptTemplate(\"{query_str}\")\n",
        "\n",
        "    llm = HuggingFaceLLM(\n",
        "        context_window=2048,\n",
        "        max_new_tokens=1024,\n",
        "        generate_kwargs={\"temperature\": 0.0, \"do_sample\": False},\n",
        "        query_wrapper_prompt=query_wrapper_prompt,\n",
        "        tokenizer_name=selected_model,\n",
        "        model_name=selected_model,\n",
        "        device_map=\"cuda:0\",\n",
        "        # change these settings below depending on your GPU\n",
        "        model_kwargs={\"torch_dtype\": torch.float16, \"load_in_4bit\": True},\n",
        "    )\n",
        "\n",
        "    st.title('Disease prediction bot')\n",
        "    if 'messages' not in st.session_state:\n",
        "        st.session_state.messages = []\n",
        "    for message in st.session_state.messages:\n",
        "        st.chat_message(message['role']).markdown(message['content'])\n",
        "\n",
        "    prompt = st.chat_input('Pass your prompt here')\n",
        "    if prompt:\n",
        "        st.chat_message('user').markdown(prompt)\n",
        "        st.session_state.messages.append({'role': 'user', 'content': prompt})\n",
        "        response = llm(prompt)\n",
        "        st.chat_message('assistant').markdown(response)\n",
        "        st.session_state.messages.append({'role': 'assistant', 'content': response})\n",
        "\n",
        "    # Setup a tunnel to the Streamlit port 8501\n",
        "    public_url = ngrok.connect(port='8501')\n",
        "    st.write(\"Public URL:\", public_url)\n",
        "\n",
        "except Exception as e:\n",
        "    st.error(f\"An error occurred: {e}\")\n"
      ],
      "metadata": {
        "id": "BX5dQcVql3ZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run /usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py [ARGUMENTS]"
      ],
      "metadata": {
        "id": "bjGydyx5mOtX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}